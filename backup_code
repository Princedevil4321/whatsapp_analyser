import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import emoji
from collections import Counter
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
import io
import sys
import json
import requests
import datetime

# --- Initial Setup for NLTK ---
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except nltk.downloader.DownloadError:
    nltk.download('vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')
except Exception as e:
    print(f"An error occurred with NLTK download: {e}")

try:
    nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    nltk.download('stopwords')
except LookupError:
    nltk.download('stopwords')
except Exception as e:
    print(f"An error occurred with NLTK download: {e}")


# --- Step 1: Data Loading and Preprocessing ---
def parse_chat_file(file_path):
    data = {'date': [], 'time': [], 'ampm': [], 'user': [], 'message': []}
    
    pattern = re.compile(r'(\d{2}/\d{2}/\d{2}), (\d{1,2}:\d{2})\s(am|pm)\s-\s(.+?):\s(.+)')

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            match = pattern.match(line)
            if match:
                date, time, ampm, user, message = match.groups()
                data['date'].append(date)
                data['time'].append(time)
                data['ampm'].append(ampm)
                data['user'].append(user)
                data['message'].append(message)
            else:
                if data['message']:
                    data['message'][-1] += ' ' + line

    df = pd.DataFrame(data)
    
    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'] + ' ' + df['ampm'], format='%d/%m/%y %I:%M %p')
    
    return df

# Set the file name
file_name = 'chat.txt'
try:
    df = parse_chat_file(file_name)
    if df.empty:
        print("The DataFrame is empty. Please check your file path and format.")
        exit()
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Make sure it's in the same directory as the script.")
    exit()

# Capture all print statements for the report
old_stdout = sys.stdout
sys.stdout = buffer = io.StringIO()

# --- Step 2: User and Message Statistics ---
print("# User Statistics")
messages_per_user = df['user'].value_counts()
print("\n## Messages per user:")
print(messages_per_user.to_markdown())

df['word_count'] = df['message'].apply(lambda s: len(s.split()))
words_per_user = df.groupby('user')['word_count'].sum()
print("\n## Total words per user:")
print(words_per_user.to_markdown())

media_messages = df[df['message'] == '<Media omitted>'].shape[0]
print(f"\n## Total Media Shared: {media_messages}")

url_pattern = re.compile(r'https?://\S+|www\.\S+')
df['links'] = df['message'].apply(lambda x: len(re.findall(url_pattern, x)))
total_links = df['links'].sum()
print(f"## Total Links Shared: {total_links}")

# New: Average message length per user
avg_word_count = df.groupby('user')['word_count'].mean().sort_values(ascending=False)
print("\n## Average Message Length per User (in words):")
print(avg_word_count.to_markdown())


# --- Step 3: Emoji Analysis ---
print("\n# Emoji Usage")
def extract_emojis(text):
    return [c for c in text if c in emoji.EMOJI_DATA]

all_emojis = df['message'].apply(extract_emojis).explode()
emoji_counts = Counter(all_emojis.dropna())

print("\n## Most Used Emojis:")
for em, count in emoji_counts.most_common(10):
    print(f"{em}: {count}")

# --- Step 4: Activity Trends & Peak Hours ---
print("\n# Activity Trends")
df['day_of_week'] = df['datetime'].dt.day_name()
df['hour'] = df['datetime'].dt.hour
df['date_only'] = df['datetime'].dt.date
df['month_only'] = df['datetime'].dt.strftime('%Y-%m')

# Daily / Monthly Trends
daily_activity = df.groupby('date_only').size()
monthly_activity = df.groupby('month_only').size()

# New: Key Events (most active days)
key_events = daily_activity.nlargest(5).reset_index()
print("\n## Top 5 Most Active Days (Key Events):")
print(key_events.to_markdown(index=False))

# Peak Chatting Hours
def get_time_of_day(hour):
    if 5 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Afternoon'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'

df['time_of_day'] = df['hour'].apply(get_time_of_day)
peak_hours = df['time_of_day'].value_counts()
print("\n## Peak Chatting Hours:")
print(peak_hours.to_markdown())

# Heatmap data creation
activity_heatmap_data = df.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)

# --- Step 5: Text Analysis (Most Common Words & Sentiment Analysis) ---
print("\n# Text Analysis")

# Most Common Words (printed, not graphed)
print("\n## Most Common Words:")
text_combined = " ".join(df['message'].dropna())
custom_stopwords = set(nltk.corpus.stopwords.words('english'))
custom_stopwords.add('Media')
custom_stopwords.add('omitted')
custom_stopwords.add('messages') 
custom_stopwords.add('p')
custom_stopwords.add('pm')
custom_stopwords.add('am')
custom_stopwords.add('m')
custom_stopwords.add('u')

word_counts = Counter(word.lower() for word in re.findall(r'\b\w+\b', text_combined) if word.lower() not in custom_stopwords)
print("Word | Count")
print("---|---")
for word, count in word_counts.most_common(20):
    print(f"{word}|{count}")

# Sentiment Analysis
analyzer = SentimentIntensityAnalyzer()
df['sentiment_score'] = df['message'].apply(lambda x: analyzer.polarity_scores(str(x))['compound'])

def get_sentiment_label(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment_label'] = df['sentiment_score'].apply(get_sentiment_label)

print("\n## Overall Sentiment Distribution:")
print(df['sentiment_label'].value_counts().to_markdown())

sentiment_by_user = df.groupby('user')['sentiment_score'].mean().sort_values(ascending=False)
print("\n## Average Sentiment Score by User:")
print(sentiment_by_user.to_markdown())

# --- New: Reply Time and Active Streak Analysis ---
# Calculate time difference between messages
df['time_diff'] = df['datetime'].diff().dt.total_seconds() / 60
avg_reply_time = df.groupby('user')['time_diff'].mean().sort_values()
print("\n# Response Analysis")
print("\n## Average Time Between Messages (in minutes):")
print(avg_reply_time.to_markdown())

# Active Streak Analysis
def find_active_streaks(user_df):
    user_df = user_df.sort_values('datetime')
    user_df['time_gap'] = user_df['datetime'].diff().dt.total_seconds()
    user_df['streak_group'] = (user_df['time_gap'] > 60 * 60 * 24).cumsum()
    streaks = user_df.groupby('streak_group').size()
    if not streaks.empty:
        return streaks.max()
    return 0

active_streaks = df.groupby('user').apply(find_active_streaks)
print("\n## Longest Consecutive Daily Streak:")
print(active_streaks.to_markdown())


# --- Generate AI Summary with LLM ---
def generate_ai_summary(data_dict):
    system_prompt = "You are a world-class AI specialized in social media and chat analysis. Your task is to provide a concise, human-readable summary of a chat conversation based on the provided data. The tone should be friendly and conversational. Focus on the relationship dynamics, communication style, and emotional tone. Do not use technical jargon like 'compound scores'. Explain the data in a way a layperson can understand. The relationship should be described in human terms (e.g., 'harmonious', 'tense', 'functional')."
    user_query = f"""
    Give a brutally honest and slightly funny analysis of this chat.
Focus on the real vibe and truth of their relationship dynamics, no sugarcoating.
Point out:
- Who started engaging more first and when the conversation picked up.
- How the relationship evolved over time (casual, flirty, toxic, besties, etc.).
- Who tends to drive or escalate conversations.
- Key emotional tones and shifts (positive, negative, neutral).
- Funny patterns or quirks in how they chat (like who spams emojis, who overshares, etc.).
Make it simple and straight to understanble by norm people in as im going to present this college,
but still keep it fun and easy to read

    - Total messages: {data_dict['total_messages']}
    - Number of users: {data_dict['num_users']}
    - Message counts per user: {data_dict['messages_per_user_markdown']}
    - Average message length: {data_dict['avg_word_count_markdown']}
    - Sentiment counts: {data_dict['sentiment_counts_markdown']}
    - Average sentiment score per user: {data_dict['sentiment_by_user_markdown']}
    - Most common emojis: {data_dict['emoji_counts']}
    - Most active day: {data_dict['most_active_day']}
    - Most active time of day: {data_dict['most_active_time']}
    - Top 5 most active days: {data_dict['key_events_markdown']}
    - Average time between messages: {data_dict['avg_reply_time_markdown']}
    - Longest active streak: {data_dict['active_streaks_markdown']}
    """
    
    apiKey = "AIzaSyBgOW5cUGSrPJ0XrTVThOdln2c_Ls5ibWE" 
    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={apiKey}"

    payload = {
        "contents": [{"parts": [{"text": user_query}]}],
        "systemInstruction": {"parts": [{"text": system_prompt}]},
        "tools": [{"google_search": {}}]
    }

    try:
        response = requests.post(apiUrl, json=payload, headers={'Content-Type': 'application/json'})
        response.raise_for_status()
        result = response.json()
        candidate = result.get('candidates', [{}])[0]
        
        if candidate and candidate.get('content', {}).get('parts'):
            return candidate['content']['parts'][0]['text']
        else:
            return "AI summary could not be generated."
    except requests.exceptions.RequestException as e:
        return f"An error occurred during the API call: {e}"

def main():
    # Gather data for the AI summary
    data_dict = {
        'total_messages': df.shape[0],
        'num_users': len(messages_per_user),
        'messages_per_user_markdown': messages_per_user.to_markdown(),
        'avg_word_count_markdown': avg_word_count.to_markdown(),
        'sentiment_counts_markdown': df['sentiment_label'].value_counts().to_markdown(),
        'sentiment_by_user_markdown': sentiment_by_user.to_markdown(),
        'emoji_counts': emoji_counts.most_common(5),
        'most_active_day': df['day_of_week'].mode()[0],
        'most_active_time': df['time_of_day'].mode()[0],
        'key_events_markdown': key_events.to_markdown(index=False),
        'avg_reply_time_markdown': avg_reply_time.to_markdown(),
        'active_streaks_markdown': active_streaks.to_markdown(),
    }

    # Print AI Summary
    print("\n# AI-Powered Conversation Summary\n")
    ai_summary = generate_ai_summary(data_dict)
    print(ai_summary)
    print("\n" + "="*50 + "\n")

    # Reset stdout
    sys.stdout = old_stdout
    
    # Save the text report to a file
    with open('chat_analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(buffer.getvalue())

    # --- Plotting All Visualizations ---
    # Create a single figure with 2 subplots
    fig, axs = plt.subplots(2, 1, figsize=(18, 16)) 

    # --- Plot 1: Monthly Message Trends ---
    monthly_activity = df.groupby(df['datetime'].dt.to_period('M')).size()
    monthly_activity.index = monthly_activity.index.astype(str)
    sns.barplot(x=monthly_activity.index, y=monthly_activity.values, color='deepskyblue', ax=axs[0])
    axs[0].set_title('Monthly Message Trends', fontsize=20, fontweight='bold')
    axs[0].set_xlabel('Month', fontsize=14)
    axs[0].set_ylabel('Message Count', fontsize=14)
    axs[0].tick_params(axis='x', rotation=45, labelsize=12)
    axs[0].tick_params(axis='y', labelsize=12)
    axs[0].grid(True, linestyle='--', alpha=0.7, axis='y')
    axs[0].margins(x=0.02) 

    # --- Plot 2: Most Active Day and Time Heatmap ---
    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    activity_heatmap_data_ordered = activity_heatmap_data.reindex(day_order)

    sns.heatmap(activity_heatmap_data_ordered, cmap='YlGnBu', annot=True, fmt='d', ax=axs[1],
                linewidths=.5, linecolor='black', cbar_kws={'shrink': .8}) 
    axs[1].set_title('Most Active Day and Time Heatmap (Message Frequency)', fontsize=20, fontweight='bold')
    axs[1].set_xlabel('Hour of Day (0-23)', fontsize=14)
    axs[1].set_ylabel('Day of Week', fontsize=14)
    axs[1].tick_params(axis='x', labelsize=12)
    axs[1].tick_params(axis='y', labelsize=12, rotation=0)

    plt.tight_layout(pad=4.0)

    # Save the figure to a PDF file
    plt.savefig('chat_analysis_report.pdf', bbox_inches='tight')

if __name__ == '__main__':
    main()
